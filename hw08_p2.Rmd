---
title: "HW08, pt 2:"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

## Load necessary libraries

```{r, message = FALSE}
library(tidyverse)
library(stringr)
library(jsonlite)
library(httr)

library(rvest)

nyt_api <- getOption("nyt_semantics_key")
guardian_api <- getOption("guardian_key")

theme_set(theme_minimal())
```

keep in mind you'll need to have your own API key for both NYT (Semantics API) & Guardian API. store it in .Rprofile w/ format of options(keyname = "actualkeycode").

## Overview
compare coverage of BLM this past yr geographically?

track coverage of gay marriage over time? since maybe 2000?

reference: https://open-platform.theguardian.com/documentation/tag

```{r}
guard_equal <- GET(
  url = "https://content.guardianapis.com/search?", 
  query = list(`q` = "equal-marriage", 
               `api-key` = guardian_api)
  )


guardjson <- content(guard_equal, as = "parsed", type = "application/json") %>% 
  as_tibble()

jsonlite::fromJSON(guardjson)

guardjson %>%
  unnest_auto(response)
#%>%
  hoist(9)


guard_equal 
```

## 

```{r}


nyt_trump <- GET(
  url = "http://api.nytimes.com/svc/semantic/v2/concept", 
  query = list(`nytd_per` = "Trump, Donald", 
               `api-key` = nyt_api)
  )

nyt_trump$url
nyt_response <- content(nyt_trump, "text")

nyt_response_df <- fromJSON(nyt_response, simplifyDataFrame = TRUE, flatten = TRUE)

str(response_df, max.level = 2)
```

```{r}
base.url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
search_term <- "'John Mearsheimer'"

r <- GET(base.url, 
         query = list(`q` = search_term,
                      `api-key` = nyt_api))

bs <- content(r, "text")
str_sub(bs, 1, 1000)

bs_df <- fromJSON(bs, simplifyDataFrame = TRUE, flatten = TRUE)

# Inspect the dataframe
str(bs_df, max.level = 2)
```





```{r asoiaf}
stark <- GET(
  url = "https://www.anapioficeandfire.com/api/houses",
  query = list(name = "House Stark")
  )

stark$url



asoiaf_houses <- function(housename){
  response <- GET(url = "https://anapioficeandfire.com/api/houses",
                  query = list(name = housename))
  
  response_df <- content(response) %>%
    as_tibble()
  
}

house <- c("Stark", "Lannister", "Targaryen")
asoiaf_houses("Stark")
```

Rachelle Terman - NYT


```{r}

```

## pivot to scraping lol
```{r}
# use SelectorGadget
pterry <- read_html("https://www.goodreads.com/author/show/1654.Terry_Pratchett")

pterry_quotes <- read_html("https://www.goodreads.com/author/quotes/1654.Terry_Pratchett")

quotes <- html_nodes(pterry_quotes, ".quoteText")

name <- html_text(quotes) %>%
  str_trim(side = "both") 

gsub("CDATA.*", "", name)
#%>%
#  str_remove_all(".CDATA*")

#str_extract


quotes1 <-html_nodes(pterry_quotes, ".authorOrTitle")

html_text(quotes1) %>%
  str_trim(side = "both")


# this works
pterry_tags <- html_nodes(pterry_quotes, ".left")

test <- pterry_tags %>%
  html_text() #%>%
#  str_trim(side = "both") 

test %>%
  str_replace_all(pattern = "tags:", "") %>%
  str_replace_all(pattern = "\n       ", " ") %>%
  str_trim(side = "both")

pterry_text <- html_nodes(pterry_quotes, "span.authorOrTitle , .quoteText")

try <- pterry_text %>%
  html_text()
try %>%
  str_replace_all(pattern = "")

#separate
```


```{r books}
books <- read_html("http://books.toscrape.com/")

books_titles_nodes <- html_nodes(books, ".product_pod a ")

titles_df <- books_titles_nodes %>%
  html_text() %>%
  as_tibble() %>%
  filter(!(value=="")) %>%
  rename(title = value)

books_prices_nodes <- html_nodes(books, ".price_color")

prices_df <- books_prices_nodes %>%
  html_text() %>%
  as_tibble() %>%
  filter(!(value=="")) %>%
  rename(price = value)

titles_df %>%
  bind_cols(prices_df)
```
```{r mangaupdates}
manga_p1 <- read_html("https://www.mangaupdates.com/stats.html")

manga_p1_nodes <- html_nodes(manga_p1, ".text-truncate span , .text-truncate u, #main_content .text.text-center")

#manga_p1_df <- 
manga_p1_nodes %>%
  html_text() %>%
  as_tibble() %>%
  filter(!value == "Rank")



```





